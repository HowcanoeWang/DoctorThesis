% \begin{eabstract}
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

\begin{doublespace}
% \paragraph{Chapter 1: Background and objectives}
% Climate change affects agriculture and food production. It is essential to improve the efficiency of crop breeding and management to ensure a sustainable increase in crop productivity for global food security. High throughput plant phenotyping plays an important role in crop management to quantitatively evaluate crop growth and its interaction with the environment. Recently, many methodologies have been developed to improve the efficiency and accuracy of plant phenotyping using remote and proximal sensing and image processing techniques. Those methods now can measure most of the phenotypic traits that can be measured in two-dimensional, such as canopy cover and organ number. However, those traits that needed to be measured in three-dimensional, such as structure and volume, remain difficult, especially in field conditions. When it is conducted in a controlled indoor environment, obtained plant 3D structure models are proven to have high quality but low throughput. However, drones for obtaining outdoor field-grown crop models are often low quality but high throughput. The low quality is often unavoidable by wind-caused blurring and canopy occlusion caused structure loss. Conducting 3D plant phenotyping for outdoor agricultural applications with both high quality and high throughput has proved to be a difficult problem.

Climate change affects agriculture and food production. It is essential to improve the efficiency of crop breeding to ensure a sustainable increase in crop productivity for global food security. Likewise, high throughput plant phenotyping that quantitatively evaluates crop growth and its interaction with the environment plays an important role in precision agriculture. Recently, many methodologies have been developed to improve the efficiency and accuracy of plant phenotyping using remote and proximal sensing and image processing techniques. These methods can now measure many phenotypic traits that are measurable in two-dimensional (2D), such as canopy coverage and organ number. However, it is still difficult to measure those traits that need to be measured in three-dimensional (3D), such as structure and volume, especially in field conditions. Recently, the photogrammetry technique has been widely used in 3D measurement in various fields. Photogrammetry can generate 3D model of an object from 2D images, it is normally combined with aerial (Long-range) or terrestrial (Close-range) sensing platforms. In agriculture, aerial sensing combined with photogrammetry can efficiently obtain the canopy 3D model of the entire field, but its quality is often not enough for 3D analysis. The quality is often affected by wind-caused blurring and canopy-occlusion-caused structure loss. Terrestrial sensing combined with photogrammetry can obtain a high-quality 3D model, but the efficiency and throughput of current approaches cannot satisfy the demand of thousands of plants in the field. Thus, outdoor 3D plant phenotyping with high quality and throughput remains a difficult challenge.

To address the challenges mentioned above, we presented a multi-scale data fusion method that combines the strengths of close-range (high quality) and aerial (high throughput) surveys. Taking field-grown broccoli as a representative of row-planted crops having harvestable organs on the top of the canopy, the thesis consisted of three studies: 

\begin{enumerate}
  \item Develop the close-range 3D-based phenotyping pipeline, which can obtain high-quality 3D plant structural models and calculate morphological traits from 1D to 3D.
  \item Develop the aerial 3D-based phenotyping pipeline, which can obtain the 3D canopy model of the entire field and improve the accuracy of 2D morphological traits by fusing with raw aerial images and time-series data.
  \item Fuse the 3D structural models from the close-range photogrammetry and the aerial photogrammetry, which can calibrate the aerial-measured 2D morphological traits and provide better 3D visualization in the field.
\end{enumerate}

\noindent
\textbf{1. Close-range 3D-based phenotyping pipeline}

The objective was to develop and validate the close-range 3D phenotyping pipeline, which obtains high-quality 3D models of destructively sampled plants (a broccoli head as an example) and calculates morphological traits. Although lots of software (photogrammetry-based) could generate the 3D model from images, users still need to conduct the process from image acquisition to parameter tuning for trait measurement. Users often deal with one plant by one manually, which is not suited for large population sizes. This chapter first presented an almost-automatic workflow that captures and saves the close-range image of the target crop in multiple view angles (perspectives). Then, \gls{roi} on the broccoli heads in the images were extracted by two pre-trained deep learning models. The preprocessed images were then fed into photogrammetry-based software (Agisoft Metashape) to generate 3D models using automatic processing scripts. Finally, the broccoli crown part was automatically segmented, the 3D model coordinates were corrected, and the phenotypic traits were calculated automatically. To evaluate the performance of the proposed pipeline, we compared some of the traits measured through the pipeline with manually measured ones. Statistically high correlations were observed between manual measurements and pipeline calculated traits ($R^2>0.96$) with \gls{rmse} $< 0.55$ $cm$. These findings indicated that the proposed pipeline has high feasibility and accuracy to achieve the final targets of the study.

\vspace{5mm}
\noindent
\textbf{2. Aerial 3D-based phenotyping pipeline}

The objective was to develop and validate the large-scale aerial 3D phenotyping pipeline using drones equipped with commercial \gls{rgb} cameras. To overcome the challenges of distinguishing small broccoli heads on low-quality 3D canopy models from photogrammetry, we proposed a novel data-fusion method. We first fused the time-series data. By using the broccoli seedling positions extracted from the early seedling stage, which is a very simple detection task, the processing regions of later broccoli head segmentation tasks were narrowed down dramatically. This method combined with active learning, significantly decreased the workload of data annotation for deep learning training and the data amount for deep learning processing. Then, we fused the data between pixel coordinates (raw images) and geographical coordinates (photogrammetry outputs). This method segmented the broccoli head region in the raw drone images, whose quality is better than the photogrammetry outputs but lacks actual scale in its pixel coordinates. To compensate for this scale missing, the projective transformation was used to convert the segmentation results to geographical coordinates in order to calculate the head size. Good correlations (0.57 $<R^2<$ 0.74) between the traits measured manually and those automatically measured through the pipeline were observed.

\vspace{5mm}
\noindent
\textbf{3. Data fusion for virtual visualization}

The objective was to test the idea of data fusion on the pipelines built in the previous chapters. Though the projective transformation used in the previous chapter solved the scale missing problem, the transformed broccoli head positions were not so accurate due to the slight perspective differences. Hence, the piecewise affine transformation was used to better locate the plant positions in the geographical coordinates. Then, automatic machine learning was trained by the destructively sampled broccoli heads, which are available in both close-range broccoli head database and the canopy 3D models from aerial photogrammetry. The trained \gls{automl} model was then used to calibrate the 2D morphological traits from the aerial pipeline. Later, a modified normalized cross-correlation template matching method was used to find the closest template in the close-range 3D model database and transform it back to the aerial 3D canopy model for virtual visualization.

\vspace{5mm}
\noindent
\textbf{General discussion and conclusions}

% This study showed that (1) the proposed indoor and outdoor plant phenotyping pipelines were feasible for crops with both simple and complex structure  , (2) The accuracy of calculated morphological traits  was valided by field measurement with high pairwise correlations, (3) the cross-scale assimilation on broccoli showed the ability to fix missing model parts and obtain more accurate traits than single outdoor scale.

Overall, the results of three research chapters showed the feasibility of the proposed 3D-based phenotyping pipelines in the tested fields. The first close-range pipeline successfully obtained high-quality broccoli head models without heavy workloads. Based on this pipeline, we built the close-range broccoli head database. The second aerial pipeline successfully obtained better head segmentation results by fusing the raw \gls{uav} images. Meanwhile, by using active learning and time-series data fusion, the workload for deep learning data annotation and processing was decreased. Lastly, the proposed \gls{automl} calibration model successfully improved the accuracy of aerial measured morphological traits; and the template matching approach fixed the structure loss of broccoli heads in the aerial 3D models for virtual visualization. The results and statistical analysis concluded that the research objectives have been achieved, and we can conclude that the research has made a positive contribution to 3D-based plant phenotyping and precision agriculture. Lastly, there is currently no strong evidence that the proposed methods can achieve similar performance in different farmland or vegetables, which is suggested for further studies.

\end{doublespace}
  
% \end{eabstract}

% \begin{jabstract}
%   概要、概要概要概要概要概要概要概要概要概要概要概要概要概要概要概要概要概要
% \end{jabstract}