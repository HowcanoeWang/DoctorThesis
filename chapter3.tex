\chapter{Aerial 3D-based phenotyping pipeline}

% \input{block/broccoli_profit.tex}

\section{Introduction}

% Waste caused by non-standard vegetables is an unavoidable component of food loss in modern society \citep{parfitt_food_2010,teuber_food_2016}. Vegetables that do not meet the cosmetic standards (e.g. size, shape, and aesthetics) cannot be easily sold and are not harvested \citep{garrone_opening_2014}. \citet{porter_avoidable_2018} estimated that over one-third of the total agricultural production (e.g., 51,500 kilotons annually in the European Economic Area) is lost for this reason. Furthermore, owing to the uneven growth rate of individual vegetables and one-time mechanical reaping, some vegetables are not harvested at the appropriate time and thus are discarded. Although the conventional method of selective harvesting by hand several times during the growing season could minimize such waste, the labor cost (107 person-hours per hectare) eliminates its profits \citep{blok_effect_2021}. Thus, the development of technologies to monitor and measure the status of each harvestable organ of the whole field is of great value.

% Waste caused by non-standard vegetables is an unavoidable component of food loss in modern society \citep{parfitt_food_2010,teuber_food_2016}. Cosmetic standards (e.g., size, shape, and aesthetics) play an important role in vegetable quality standards in Northern Hemisphere countries \citep{porter_avoidable_2018}. Therefore, vegetables that do not meet this standard cannot be easily sold and are not harvested \citep{garrone_opening_2014}. \citet{porter_avoidable_2018} estimated that over one-third of the total agricultural production (e.g., 51,500 kilotons annually in the European Economic Area) is lost for this reason. For vegetables harvested by mechanical reaping, owing to the uneven growth rate of individual vegetables, some are not harvested at the appropriate time and are discarded. Hence, the development of technologies to predict the optimal time for the whole field harvesting not only increases the effective yield and profit of farmers, but also contributes to sustainable development and the global environment (Sustainable Development Goals [SDG] targets 12.3 and 12.5).

% Broccoli (\textit{Brassica oleracea} L.) head is an important component of the global vegetable market; however, there is a high percentage of on-farm waste. Its non-edible parts (leaves, stems) account for $> 75\%$ of the above-ground biomass \citep[Table~1]{fink_nitrogen_1999}. For the remaining marketable parts, the variable bud growth rate results in large variations in head size under complex field conditions. As for other vegetables, fresh broccoli has shipping standards (head diameter, weight, shape, etc.); therefore, a certain amount of non-standard harvested head is wasted from one-time mechanical harvesting. Although the conventional method of selective harvesting by hand three times during the growing season could minimize such waste, the labor cost (107 person-hours per hectare) eliminates its profits \citep{blok_effect_2021}. Because the shipping price is highly dependent on head size, the harvest date is essential to determine the proportion of non-standard-size broccoli and the total income for farmers. If the size distribution of all individuals in the broccoli field could be determined and predicted in the short term, it would be possible to set the optimal harvest date to reduce the number of non-standard-size vegetables and minimize food losses. However, it is unrealistic to manually determine the size distribution of all broccoli in the field.

Smart farming, which involves new technologies such as remote sensing, high-throughput phenotyping, and artificial intelligence in agricultural production, has received considerable attention from researchers, farmers, and governments. The \gls{uav}-based aerial pipeline provides a cost-efficient method to capture images for crop canopies. The 3D canopy model can also be reconstructed using photogrammetry-based software. Several studies extended this approach to estimate canopy architectural traits \citep{shu_application_2021, wang_detection_2021, herrero_canopy_2020} and even traits for individual lettuce plants \citep{bauer_combining_2019}. But such trait phenotyping at the canopy level or individual level cannot meet the accuracy demand for harvestable organ size judgment; for example, broccoli head grades vary by a few centimeters. Therefore, extracting the organ-level traits can significantly improve the accuracy of whole-field estimation.

For many organ-level applications, the images are often collected close to the ground (less than 1 $m$ between a sensor and a plant) by a hand-held camera or an \gls{ugv}. Although several studies successfully proved the accuracy and feasibility of this approach on small-area experimental fields \citep{luling_using_2021,garcia_towards_2021,blok_effect_2021}, its efficiency is not always applicable to the large-area field with thousands of individuals. To apply the organ-level analysis on the \gls{uav}-based approach, there are two main challenges to be solved.

One challenge is the poor quality of the reconstructed canopy model (2D field map and 3D point cloud). Due to the plant structure movement in different \gls{uav} images caused by wind, the canopy model often has the effects of double mapping (ghost effect) and seamline distortion \citep{lin_new_2021} (Figure~\ref{fig:idp1}b). Many studies tried to fix the low quality using machine learning algorithms \citep{hu_pixel_2019,hu_coupling_2021,velumani_estimates_2021} or multi-spectral sensors \citep{guo_wheat_2021,lu_assessment_2022}, these can be time-consuming, costly, and are often not robust on different crops. Little attention has been paid to the original \gls{uav} images which often have better quality. Without pixel-level \gls{gps} coordinates on original images, it is not possible to accurately match plants from images to corresponding locations in the field. The solution is to reuse the intermediate parameters during the 3D reconstruction. In the 3D reconstruction procedure, the relative rotation angle and position between the \gls{uav} images and the object (field) have been calculated and calibrated by the \gls{gcp} (Fig.~\ref{fig:idp1}c). Therefore, the transformation matrix from image pixel coordinates to real-world 3D coordinates is available from the intermediate parameters. \citet{duan_comparison_2017} and \citet{guo_aerial_2018} tested this idea to find the real-world \gls{roi} on corresponding original \gls{uav} images, while \citet{lin_new_2021} developed their algorithms to fix the field map. But they did not publish any source code or tools. It is difficult and time-consuming to reproduce and use these research achievements directly, especially for those who are not familiar with professional photogrammetry programming. So the previous research outputs remain far from the development of user-friendly tools. %making it unreachable to agricultural users who do not have solid photogrammetry backgrounds and professional programming skills.

\input{figures/idp.fig.1.tex}

The other challenge is the complexity of crop images. As Figure~\ref{fig:idp1}a shows, there are huge differences in time, sunlight, soil condition, growing stage, and cultivar. It is quite difficult to design a conventional computer vision algorithm to handle all types of variation at the same time. Many studies have applied deep learning algorithms on the broccoli head image segmentation tasks \citep{garcia_towards_2021,blok_image_2021,zhou_monitoring_2020}, but most of them took images under controlled conditions (indoor or inside a black box). To fulfill the more complex outdoor tasks, a large amount of training data need to be manually labeled. Though there is a public training dataset for cauliflower available \citep{kierdorf_growliflower_2022}, it cannot be used directly on the broccoli head or another crop. Besides, labeling 14,000 individual plants manually as in the cauliflower dataset is not feasible for all kinds of crops. To decrease the workload of this deep learning solution, it is preferable to simplify the image segmentation, minimize the number of computation tasks, and increase the efficiency of training data acquisition as much as possible.

% Smart farming, which involves new technologies such as machinery, remote sensing, high-throughput phenotyping, and artificial intelligence in agricultural production, has received considerable attention from researchers, farmers, and governments. The \gls{uav}-based pipeline provides a flexible and cost-efficient method to capture high-resolution images using various lightweight sensors. By using high-resolution \gls{uav} images captured over time, it may be possible to estimate the growth of the head size of all individual vegetables. The time-series data of the head size distribution of all broccoli can be used to develop a prediction model for the short-term growth of broccoli heads. If the head size of all individuals can be predicted, combined with the market prices for each size grade, a prediction system for the optimal harvest date can be built. 

% However, several challenges must be overcome to develop a broccoli head size estimation pipeline. First, broccoli grades vary by a few centimeters; therefore, a highly accurate estimation is required. In particular, for \gls{uav} images from the broccoli field, the head can be hidden by leaves. Second, for agricultural field applications, it is preferable to minimize the number of computation tasks as much as possible. In particular, semantic segmentation for each crop or organ was obtained from images of the entire field using deep learning \citep{bauer_combining_2019,zhou_automated_2022}. Third, deep learning model training is often powered by a large amount of training data and annotations. Efficient acquisition of these data is an urgent need for deep learning applications in agriculture \citep{kierdorf_growliflower_2022}.

In this chapter, we aimed to develop several techniques (backward projection, pre-position-guided head segmentation, and interactive annotation) to overcome the aforementioned challenges and provide a highly accurate and labor-saving pipeline for broccoli head size estimation. The objectives were to: (1) develop a general workflow of the broccoli head size estimation pipeline; (2) implement the pipeline for field trials of broccoli growth in 2020 and 2021; (3) validate the estimated head size by comparison with field measurements; and (4) open all the source code of this pipeline to the public domain. %This pipeline has shown the ability to guide optimal harvest time and increase profitability \cite{nishida_estimation_2023}. This pipeline also has great potential to be seamlessly interfaced with other cabbage-like crops, including cauliflower, artichoke, and even lettuce. Meanwhile, the use of a simple \gls{rgb} sensor, not a complex integration of multiple or expensive sensors like \gls{lidar} or multi-spectral camera, makes it more applicability and user-friendly for the farmers, farming and the economic sustainability of many economically and socially disadvantaged rural regions. 

\section{Materials and Methods}

The general workflow of the broccoli head size measurement pipeline is shown in Fig.~\ref{fig:bro4} and the Supplementary Video \ref{spp:video}. The time-series data of all broccoli were collected and visualized using a \gls{uav}-based pipeline. The pipeline included the following steps:1) aerial data collection; 2) data preprocessing and 3D reconstruction; 3) broccoli position detection using YOLO v5 at the seedling stage; 4) broccoli head segmentation using BiSeNet v2, and; 5) geometry trait calculation. 

% Second, a simple growth model was built using the head size and temperature data. Finally, a profit prediction model was generated according to the market price survey.

\input{figures/bro.fig.4.tex}

\subsection{Plot conditions and field data collection}

Field trials were conducted at the experimental farm of the \gls{isas}, Nishi-tokyo, Tokyo, Japan ($35^\circ 43'$N, $139^\circ 32'$E) in 2020 and 2021 (Fig.~\ref{fig:bro5}). Detailed meteorological data during the growth period were collected by a local weather station and are shown in Supplementary Table ~\ref{tbl:bros2}. The plot sizes were approximately 0.2 and 0.1 $ha$ for 2020 and 2021, respectively. During the 2-year experiment, the same broccoli cultivar (Jet dome) was planted under the same field management. Machine planting of seedlings at 35 $cm$ intervals in rows 70 $cm$ apart is consistent with local commercial broccoli cultivation regimes \mbox{\citep{nishida_estimation_2023}}. 

\input{figures/bro.fig.5.tex}

The field data of the broccoli head size were measured by \mbox{\citet{nishida_estimation_2023}} as ground truth for validation. This was conducted manually every 2‒3 d using both destructive and non-destructive measurement methods. Non-destructive measures were conducted directly in the field and destructive measures were conducted indoors. In 2020, the maximum broccoli head length (as head diameter) was measured by the visual judgment of the longest axis. A total of 360 ($120 \times 3$ times) non-destructive and 434 destructive measurements of 7438 individual broccoli were recorded. 

In 2020, the potential of the proposed algorithm was identified. To further validate this algorithm, we improved the measurement method and increased the number of field samples in 2021. The maximum head length (as head diameter) was measured using the maximum value of the length in the $0^\circ$, $45^\circ$, $90^\circ$, and $135^\circ$ directions. To increase the variation in the broccoli head for each survey, there was an 8-day interval between seeding in the western and eastern parts (timeline in Fig.~\ref{fig:bro5}, yellow half circle). A total of 2,000 ($400 \times 5$ times) non-destructive and 557 destructive measurements of 3276 individual broccoli were \mbox{\citet{nishida_estimation_2023}}. To reduce the workload, only half of the area (east or west) was measured on a certain day (timeline in Fig.~\ref{fig:bro5}, blue half circle). Head diameter measurements ranged from 2‒25 cm.

\subsection{Data collection and preprocessing} \label{sec:cp3data}

After transplanting, several \gls{gcp} boards (8 for 2020 and 9 for 2021) were set at the four corners and within the field for \gls{uav} data collection. This was an important resource for later 3D reconstruction and time-series alignment. All \gls{gcp}s were measured using hemisphere \gls{rtk} differential \gls{gnss} devices to obtain geographical coordinates. For developing regions without \gls{rtk} services, the \gls{gcp}s' coordinates can be replaced by measuring distances (as scalebar corrector) among each \gls{gcp} and building a referencing map at the very beginning. The relative coordinates of those \gls{gcp}s on the referencing map can function the same as the actual geographic coordinates for time-series alignment.

Aerial images were collected by \gls{isas} technical staff using a DJI (Shenzhen DJI Technology Co., Ltd. China) Phantom 4 v2 (camera model FC6310s), a DJI Mavic 2 Pro (camera model Hasselblad) in 2020, and a DJI Phantom 4 \gls{rtk} (camera model FC6310R) in 2021. The image resolution was the same at $5472 \times 3648$ pixels. The flight height in 2020 was initially 15 m and then decreased to 10 m when the broccoli head turned up. The flight height in 2021 was constantly 15 m. Most of the flights were conducted before the field operation, except on May 22 and 26, 2020. On both these days, the destructively sampled broccoli did not exist in the \gls{uav} image; hence, the destructive data were linked to the previous flight (the black broken lines in the timeline in Fig.~\ref{fig:bro5}). For all other sampling events, data collected on the same day were paired together.

The configuration of the computer for 3D reconstruction was as follows: Intel  i9-7980XE \gls{cpu} \@2.6GHz, 64GB \gls{ram}, two NVIDIA GeForce GTX 1080Ti \gls{gpu}s, and Windows 10 Pro 64-bit operating system. Pix4DMapper Pro (Pix4D, S.A., Prilly, Switzerland) was used to perform flight investigations. The default software parameters were used and the \gls{gcp}s were marked manually on the closest 3‒5 images. The reconstruction parameters, \gls{dom}, and \gls{dsm} were produced for later use. 

The open-source software package QGIS (\url{www.qgis.org}) was used to prepare and modify GIS shapefiles, such as field boundaries and field grids on the field map (\gls{dom}). The field boundary (plot area) was a rectangular region that tightly wrapped around the broccoli (Fig.~\ref{fig:bro5}). It could be used to filter the noise outside the broccoli plot. The long boundary edges were parallel to the ridge direction (Figs.~\ref{fig:bro5} and ~\ref{fig:bro6}b). Grid plots (yellow squares in Fig.~\ref{fig:bro6}b) shared the same direction as that of the broccoli ridge and overlapped the boundary. For each grid, the edge length was 2.5 m (approximately 1000‒1500 pixels) and contained approximately 50‒100 broccolis. 

\input{figures/bro.fig.6.tex}

The LabelMe annotation tool (\url{https://github.com/wkentaro/labelme}) was used to label the training data for the deep learning models. EasyIDP (\url{https://github.com/UTokyo-FieldPhenomics-Lab/EasyIDP}) was developed and used to locate and crop the same field region imagery on the original \gls{uav} images when the \gls{dom} was not sufficiently clear for head segmentation (also known as backward projection or reverse calculation, see the Supplementary material \ref{spp:backward} for details about the methodology).

\subsection{Seedling position detection} \label{sec:detect}

The difficulty in detection varied throughout the broccoli growth season. Detection during the seedling stage was simpler than that during the flowering stage, and the latter had complex leaf occlusion. The seedling position of the broccoli was almost the same as that of the broccoli head. Aligned by the \gls{gcp}s, the positions were consistent through the time-series \gls{dom} and \gls{dsm} of the full growing season. Information of the positions detected during the early stage was directly used for the image analysis during the flowering stage (pre-position-guided), in order to reduce the difficulty in data analysis of each broccoli head.

The flight at approximately 1 month after transplanting was used to detect seedling positions (Fig.~\ref{fig:bro5}, dark green circle in April). During this period, most broccoli leaves were sufficiently large to be clearly observed from the DOM, but the leaves did not overlap. The uniform light conditions and differentiation between leaves and backgrounds were also taken into consideration when selecting the most suitable detection time.

The broccoli detection procedure is illustrated in Fig~\ref{fig:bro6}a. The full \gls{dom} was first split into several small pixel grids (named `sectors', Fig.~\ref{fig:bro6}a) along the \gls{dom} pixel matrix as the input of YOLO v5 (\url{https://github.com/ultralytics/yolov5}) by EasyIDP. The edge length of each sector was 1300 pixels and was buffered with 200 pixels (gray L-shape in Fig.~\ref{fig:bro6}a) on the lower right corner ($1300 \times 1300$ to $1500 \times 1500$) to avoid individual division on sector edges. Training data with only two sectors were labeled using LabelMe (Fig.~\ref{fig:bro6}a). 

Subsequently, the YOLO v5 detection model with default settings was trained and applied to all sectors. Outliers and noise outside the broccoli field were filtered by the plot area boundary. The duplicated buffer zone detection results were merged using the \gls{nms} algorithm. The center point of the bounding box was then viewed as the broccoli position. Finally, we manually inspected and adjusted the results in QGIS, ensuring no missing or duplicate detections (Fig.~\ref{fig:bro6}a). The broccoli ID was given from the north to the south of each ridge and ridges from the west to the east by the ridge detection algorithm (Fig.~\ref{fig:bro6}a; please refer to the links in the data availability section for further details). 

\subsection{Broccoli head segmentation} \label{sec:seg}

Leaf movement and occlusion often cause \gls{dom} with double mapping, excessive pixelation, and seamline distortions \citep{lin_new_2021}. It is difficult to obtain a \gls{dom} that meets the consistent quality of the entire field for head size estimation. To manage this situation, the same region on the raw \gls{uav} images (rather than that on the DOM) was used for analysis by backward projection (Supplementary material \ref{spp:backward}). To reduce the workload of segmentation model training, only the area around the broccoli seedling positions was used. Meanwhile, interactive annotation \citep{ghosal_weakly_2019} was applied to decrease the workload of label annotation. 

The general workflow for this section is illustrated in Fig.~\ref{fig:bro6}b. For image data preparation, only flights with visible broccoli heads were chosen for all 12 flight investigations over both years (Fig.~\ref{fig:bro5}, dark green circles in Mays). The plot area was divided into grids {fig:bro6}b1). For each grid, the grid boundary and broccoli positions were backward-projected onto the closest raw aerial image (Fig.~\ref{fig:bro6}b2). Each raw aerial image was cropped into small sectors (1500 $\times$ 1500 px) which were located in the center and contained broccoli seedling positions (Fig.~\ref{fig:bro6}b3). Only the square area (approximately 100 px side length) buffered from the seedling positions was used for broccoli head segmentation (Fig.~\ref{fig:bro6}b4) to eliminate the effects of soil and weeds.

Considering the efficiency of the interactive annotation, the deep learning model can be trained and applied in just a few minutes. Therefore, BiSeNet v2 \citep{yu_bisenet_2020} was used as the segmentation model. BiSeNet v2 is a network structure that employs multiple branches to process inputs of different sizes to strike a balance between efficiency and computational cost \citep[Fig.~1]{yu_bisenet_2020}. The BiSeNetV2 network used in the present study was based on the \url{https://github.com/CoinCheung/BiSeNet} GitHub project. We used both geometric (G) and photometric (P) transformations in data augmentation, as per \citet{blok_effect_2021}. The G strategy consisted of \textit{ShiftScaleRotate} (shift limit = 0.5, scale limit = 0.2, rotate limit = 90) and \textit{VerticalFlip}, which were used to solve the problem caused by our point-based or position-guided segmentation workflow. Ideally, the input images had only one broccoli in the middle of the input image, but in actual practice, the broccoli head position appeared randomly caused by bud-head growth shifting, or even with the probability of multiple or no broccoli heads in the input images. The P strategy simulated ``cloudy, sunny" and ``day, night" transitions using \gls{rgb} shift (r shift limit = 25, g shift limit = 25, b shift limit = 25) and \textit{RandomBrightnessContrast} (brightness limit = 0.3, contrast limit = 0.3) to address the effects of different weather and light conditions.

Interactive annotation was used to decrease the workload of image labeling \linebreak (Fig.~\ref{fig:bro6}b5). Initially, a small number of startup training data (approximately five to ten broccoli heads per flight) were manually marked using LabelMe; then, the segmentation model was trained using the startup data. Next, images were randomly selected and applied to the segmentation results. These results were transformed into LabelMe JSON formats using Python scripts. Subsequently, manual adjustment produced annotations as new training data in LabelMe. The previous steps were iteratively repeated until no significant adjustment was required for the newly applied results.

The verification dataset for the model performance evaluation was also prepared using the previous interactive annotation. The modified intersection of union (IoU) was used as the evaluation metric. In this case, only the segmentation results inside the grid region (Fig.~\ref{fig:bro6}b6, red polygon inside the yellow square) were chosen as the final results. The segmentation results attached to the grid bottom and right edge were also removed because of duplication with the neighboring grids. Here, we renamed the modified IoU inside the grid region as ``mid-IoU" (middle IoU).

The segmentation model was first trained using only the 2020 dataset for several iterations until a good performance was achieved (no manual modification was required for the model segmentation results). The model was then applied to the 2021 dataset over several iterations. When the model performed well in both years, it was applied to all dataset images, and the segmentation results after the grid boundary filter were saved for the next procedure.

\subsection{Phenotyping of head traits}\label{sec:pheno}

The unit of the segmentation polygon in raw aerial images was a pixel, which could not represent the actual head size. However, the actual head size could be calculated using the pixel scale bar from the geo-referenced DOM. The pixel scalebar on the raw images could be derived from the ratio between the grid size in pixels on the raw image and that on the DOM. This concept was then implemented using projective transformation in scikit-image (\url{https://scikit-image.org}). On some locations, one broccoli may have had duplicated polygons; only the polygon with the largest area was retained, which was accelerated by a \gls{kdtree} in SciPy (\url{https://scipy.org}).

For each polygon of broccoli head, the following geometry traits were calculated: 1) max length and min length, from the side lengths of the minimum area bounding box; 2) perimeter, circularity and area of polygon; 3) area of polygon convex hull; 4) major length, minor length and eccentricity of ellipse has the same second-moments as the polygon. 5) equivalent diameter of circle has the same area as the polygon.

The source code used in this manuscript can be accessed at \url{https://github.com/UTokyo-FieldPhenomics-Lab/UAVbroccoli}. %All original \gls{uav} image data can be accessed by Google Drive upon request (224GB for 2020 and 72GB for 2021). Although the data used in this manuscript were obtained using Pix4D, we updated the workflow to Metashape, which is easier for batch processing in codes since 2022.

\subsection{Field validation for head traits}

To test the correlation between field-measured length [dependent variable contributed by \mbox{\citet{nishida_estimation_2023}} ] and aerial pipeline measured length (independent variable contributed by myself), the coefficient of determination ($R^2$) using simple linear regression was used as the evaluator. To assess the trends in differences in broccoli size in detail, \gls{lowess} regression and distribution comparison were also used.

\section{Results}

\subsection{Broccoli position detection}

To provide a general understanding of this procedure, Supplementary Figs.~\ref{fig:bros1}a to f shows some intermediate results during broccoli position detection. The \gls{dom} of the entire broccoli plot was first divided into several small sectors with a buffer zone. All broccoli heads were labeled using bounding box (rectangle) annotation in LabelMe (Supplementary Fig.~\ref{fig:bros1}a, example sector). The detection model performed as expected in the other sectors (Supplementary Fig.~\ref{fig:bros1}b: one example sector). The green mask in Supplementary Fig.~\ref{fig:bros1}b shows the buffer zone that overlapped with neighboring sectors, with the aim of avoiding incomplete broccoli detection on the sector boundary. When merging the detection results for all sectors, duplicate detections were removed by the \gls{nms} algorithm (black rectangles in Supplementary Fig.~\ref{fig:bros1}c). When adjusting the zoom to the full map view, the removed black duplicates were clearly distributed on the sector boundaries (grid lines). The center of the detected bounding box was viewed as the broccoli position; however, it required manual inspections to correct false detections and missing broccoli. In Supplementary Fig.~\ref{fig:bros1}d, the green dots show the manually corrected (removed) YOLO detection results, and the red dots indicate the final positions used by manual adjustment. Supplementary Figure~\ref{fig:bros1}e shows the results of ridge-line detection, and Supplementary Fig.~\ref{fig:bros1}f shows some results of the automatic broccoli ID assignment. In general, broccoli position detection and semi-automatic labeling functioned as expected.

\subsection{Broccoli head segmentation}

To clearly demonstrate the interactive annotation procedure, Supplementary \linebreak Figs.~\ref{fig:bros1}g to i and Supplementary Table~\ref{tbl:bros1} provide example images and a simple summary from the first to the last iteration. As startup training data, one image was randomly selected from each flight investigation in 2020, and only a few representative broccoli heads were annotated as simply as possible (Supplementary Fig.~\ref{fig:bros1}g). The BiSeNet model (v0) was then trained using this annotation. For each flight in 2020, one image was randomly selected and applied to the v0 model (Supplementary Fig.~\ref{fig:bros1}h, red mask). The results were manually adjusted and saved as new training data (Supplementary Fig.~\ref{fig:bros1}h, blue boundary). The previous steps were iteratively repeated until the model achieved good segmentation results, and this version (in this case, v2) was used to produce 30 validation data with low labor costs. The model performance was evaluated using validation data.

The detailed model performance for each iteration version is presented in Supplementary Table~\ref{tbl:bros1}. With the proposed guidance of the broccoli position and the buffered mask, even a startup with only a few annotations could achieve a mid-IoU of 78.15\%. The model performance improved considerably after four iterations and achieved a mid-IoU of 88.33\% after the 4th iteration (Supplementary Fig.~\ref{fig:bros1}i). Then, when the v4 model trained by 2020 data was applied to the 2021 data, the performance decreased to 79.16\%, as expected. However, it significantly improved to 91.70\% after one additional iteration with six training data points added from 2021. Meanwhile, broccoli head segmentation was more challenging at an early stage (May 18, 2020, and May 12, 2021, with the lowest mid IoU) when the head size was small. This suggested that weakly supervised annotation improved the model performance and decreased the workload in data annotation with iteration.

\subsection{Head diameter validation}

The full map of all calculated HDs from the \gls{uav} is shown in Supplementary Fig.~\ref{fig:bros2}. We found a high correlation between the maximum length of the broccoli head measured (as head diameter) by the \gls{uav} image and that measured manually in the field (Fig.~\ref{fig:bro1}). The \gls{uav} method tended to underestimate broccoli growth (trend line above the reference line). However, the overall distribution of the estimated head size was almost the same between the two groups (\gls{uav} vs. manual, Figs.~\ref{fig:bro1}c and d). 

\input{figures/bro.fig.1.tex}


\section{Discussion}

The main aim of this chapter was to test the use of \gls{uav}-mounted digital measurements of broccoli head size and their use in monitoring for different sellable size classes. Here, we could successfully create a broccoli head size estimating pipeline based on \gls{uav} imagery, \gls{mldl} for the entire broccoli field. We demonstrated that the head sizes estimated by \gls{uav} imagery were highly correlated with the field measurements. Although the current chapter focused on broccoli as a model crop, this framework could be readily applied to other crops such as cauliflower, artichoke, and cabbage. Thus, our case study shows that smart farming techniques have great potential to contribute to the sustainable development of vegetable production.

% Reducing on-farm food loss (e.g., non-standard-sized vegetables) is one of the prominent goals of sustainable development in agricultural production. The main aim of this work is to test the use of \gls{uav}-mounted digital measurements of broccoli head size and their use in monitoring and prediction of optimal harvest time in terms of economic returns for different sellable size classes. In this study, we created a harvest date prediction system based on \gls{uav} imagery, machine learning / deep learning (ML/DL), and a growth model by predicting the short-term change in the head size of all individuals ($> 3000$) in the entire broccoli field. Our experiments demonstrated that: (1) the head sizes estimated by \gls{uav} imagery were highly correlated with the field measurements; (2) the proportion of non-standard-size and the total income calculated by the hypothetical harvesting changed dramatically between harvest days; (3) predictions for the few days following a particular date of \gls{uav} imagery were highly correlated with those estimated by \gls{uav} imagery taken on that date; and (4) the optimal harvest date (i.e., the date for the minimum proportion of non-standard-size broccoli and maximum income) could be predicted with high accuracy. These results suggested that our prediction system for the optimal harvest date of broccoli will benefit farmers by reducing food loss and increasing their income. Although the current study focused on broccoli as a model system, this framework could be readily applied to other similar vegetables such as cauliflower, artichoke, and cabbage. Thus, our case study shows that smart farming techniques have great potential to contribute to the sustainable development of vegetable production.

% This study showed that the proportion of non-standard-size broccoli and the total income changed rapidly depending on the harvest date. For example, 1 day later or earlier than the optimal harvest date increased the number of non-standard-size broccoli by approximately 5\% and decreased the total income by approximately 20\%; and 2 days later than the optimal harvest date increased the number of non-standard-size broccoli by approximately 15\% and decreased the total profit by approximately 40\%. To the best of our knowledge, such temporal changes in the number of non-standard-size vegetables and the total profit for different harvest dates have not previously been calculated because no technique was available to measure thousands of individual vegetables multiple times with high accuracy. Interestingly, the optimal harvest date was largely unaffected by differences in the shipping price for each grade (Figs.~\ref{fig:bro3}c and d). The optimal harvest date was determined by the spatial variation in broccoli growth, regardless of the shipping price of each grade. The difficulty in setting the harvest date for mechanical harvesting owing to large spatial variation is a common issue in broccoli and other vegetable farms. Thus, predicting the optimal harvest date using our system (or similar systems) has the potential to reduce on-farm food loss and increase the income of vegetable farmers worldwide.

In addition to estimating temporal variations in head size distribution, our pipeline could visualize spatial variations in individual head size (Supplementary Fig.~\ref{fig:bros2}). 
%In the 2021 trial, spatial variation in broccoli size was intentionally created by planting seedlings on two different dates in the eastern and western areas of the field (8 days intervals). When we visualized the individual head sizes, the difference between the eastern and western fields was evident. 
However, it was difficult to visually observe the differences on the ground. 
%For example, the difference in average head size between the east and west on March 12, 2021, was only 3 cm. Although the uneven growth between the west and east fields in this trial was intentional, 
Such a spatial variation can often occur on field-grown crops, especially in large-scale agriculture \citep{quine_investigation_2002}. %In such a large-scale uneven field, farmers can divide their field into several areas and harvest broccoli multiple times. Based on this extended study using our pipeline \citep{nishida_estimation_2023}, farmers may be able to visualize the spatial variation of their fields (Supplementary Fig.~\ref{fig:bros2}), predict their short-term growth, and determine the optimal spatial and temporal harvest strategy.

Although the pipeline we developed highlights the benefits and importance of \gls{uav} imagery powered by \gls{mldl} for sustainable agricultural development, there are some limitations to its use. 
First, our system is neither fully automated nor app-based; therefore, farmers without computer science backgrounds cannot use this system directly in their own fields. However, because the source code is open to the public (\url{https://github.com/UTokyo-FieldPhenomics-Lab/UAVbroccoli}), local agricultural institutes and agricultural companies are able to modify and use the system according to their target. This study is definitely not a one-stop solution, but is a pioneer in real agriculture applications. 
Second, unlike traditional manual methods with limited throughput, the proposed method can sample every plant in the field at much higher frequency, leading to higher temporal and spatial resolution. The large amount of data generated by this method is therefore appropriate for data-driven modeling, which could lead to breakthroughs in smart farming.
Third, manual inspection of the seedling position detection is required; this step cannot be omitted because this result is the basis for subsequent broccoli segmentation. Detection omissions, duplications, and drifts need to be checked manually on a case-by-case basis. Although it saves considerable effort compared to adding them manually one by one, this inspection still requires several hours to complete in large-scale fields. Additionally, if a broccoli plant dies before the flowering stage, there is a risk of wrong head segmentation generating incorrect results (but in some cases that we observed, it gave an empty segmentation result and was easily discarded). 
Fourth, the problem of leaf occlusion has not been solved, which remains a challenging problem for plant phenotyping \citep{zhang_applications_2020}. As broccoli heads are essentially round, one approach is to restore the roundness of the stubs. The circularity and eccentricity of the broccoli head may be used to describe the severity of occlusion. The least squares for round fitting can be used, or the deep learning framework \gls{orcnn} can be applied to obtain improved recovery results \citep{blok_image_2021}. However, it requires a depth camera and image pairs before and after occlusion as training data is collected on the ground, which is inconvenient for current \gls{uav} applications but warrants further study. For example, multi-spectral and even \gls{lidar} sensors are becoming increasingly cheap, combined with the rapid development of \gls{ai} algorithms, suggesting this problem could be resolved without unaffordable cost increases. Also, integration of the method with other common management practices such as mulching films with bioplastics could assist in the identification of plants and broccoli heads, particularly when used in conjunction with a multi-spectral sensor. 
Finally, hardware and software instrument costs should not be omitted. The \gls{uav} with \gls{rtk} (\$6,500), 3D reconstruction software (\$3,499), and a high-performance computer (\$6,000) for computation employed in this chapter would limit the pipeline's widespread use. However, even for a small farm (0.2 ha) with 7,000 broccoli plants, only 2-day difference from the optimal harvest date can result in an income loss of almost \$2,000. The feasibility of our pipeline on larger farms is also worth being tested in the future. For companies that provide this type of agricultural consulting service, this one-off investment can be offset by the increased profit of many producers. For those economically and socially disadvantaged rural regions, the \gls{rtk} or the expense of a base-station should not be mandatory. It can be replaced by setting more \gls{gcp} boards and measuring distances among them as scalebar correctors, to get similar results with relative geographical coordinates. It was suggested that cooperating with local broccoli farmers to test the proposed system without \gls{rtk} dependences and keeping improving the algorithm performance on the occlusion area will be needed in futher work. The head quality and transport costs were also suggested to be integrated into the system to refine its applicability.

%\hspace*{\fill}

%\noindent \textbf{Acknowledgments}: We thank the technical staff of the \gls{isas} for the management of broccoli fields. 

% \hspace*{\fill}

% \noindent \textbf{Author Contributions}: W.G. and Y.F. designed the study. E.N. managed the broccoli field and collected field data. W.G. (and technical staff) collected the \gls{uav} images and 3D reconstructed the plots. H.W. created the full \gls{uav} image analysis pipeline, and T.L. contributed to the deep learning and interactive annotation coding work. E.N. built the prediction models for income. H.W. and E.N. prepared the figures. H.W., E.N., and T.L. prepared the manuscript. Y.K., W.G., and Y.F. supervised this study.

%\hspace*{\fill}

%\noindent \textbf{Funding}: This study was partially supported by the Japan Science and Technology Agency (JST) AIP Acceleration Research (JPMJCR21U3), by the Ministry of Agriculture, Forestry and Fisheries (MAFF) ``Research project for technologies to strengthen the international competitiveness of Japan's agriculture and food industry."

% \hspace*{\fill}

% \noindent \textbf{Competing interests}: The authors declare no competing interests nor any potential conflict of interest.

% \hspace*{\fill}

% \noindent \textbf{Data Availability}: The source code used in this manuscript can be accessed at \url{https://github.com/UTokyo-FieldPhenomics-Lab/UAVbroccoli}. All original \gls{uav} image data can be accessed by Google Drive upon request (224GB for 2020 and 72GB for 2021). Although the data used in this manuscript were obtained using Pix4D, we updated the workflow to Metashape, which is easier for batch processing in codes since 2022.


\appendixsection

\input{appendix/easyidp_method.tex}

\subsection{Supplementary video}
\label{spp:video}

An illustration video about the general pipeline of this study can be accessed here \url{https://youtu.be/SYuOCVqgtrU}. The background music used in this video is copyright-free music from \url{freepd.com}.

\subsection{Supplementary figure and tables}

\input{tables/bro.tbl.s1.tex}

\input{tables/bro.tbl.s2.tex}

\input{figures/bro.fig.s1.tex}

\newpage

\begin{landscape}
  \input{figures/bro.fig.s2.tex}
\end{landscape}

% \newpage

% \begin{landscape}
  % \input{figures/bro.fig.s3.tex}
% \end{landscape}