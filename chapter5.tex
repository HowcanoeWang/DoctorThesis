\chapter{General discussion}

\section{Conclusions and reflections}

This is the first study that combined the advances of aerial (high throughput) and close-range (high quality) surveys in plant phenotyping applications to field-grown crops. This Ph.D. thesis aimed to improve the performance of 3D-based plant phenotyping on field-grown broccoli as a representative of row-planted crops having harvestable organs on the top of canopy, using only low-cost \gls{rgb} cameras. Using \gls{uav} for aerial surveys and photogrammetry allows for the efficient acquisition of 2D field maps and 3D models of the crop canopy for the entire farmland. However, due to limitations in survey efficiency and wind blurring caused by propellers, the \gls{uav} cannot fly too close to plants, resulting in inadequate resolution and quality for directly analyzing the broccoli heads at the organ level. This thesis attempted to fuse the close-range and aerial 3D phenotype data, as well as some latest machine learning and deep learning techniques, to accurately and efficiently obtain the position and size of broccoli heads in the field. Furthermore, this thesis also provided a better 3D virtual visualization for those broccoli heads in the field and builds a foundation for digital twins and virtual farmland for smart agriculture. 

% cp 2-4
We first developed an almost automated close-range pipeline to obtain the high-quality 3D models of destructively sampled broccoli heads, as a template database. By using the dual-rotation methods and the automated rotation platform, images from different perspectives of broccoli heads for 3D reconstruction were collected without heavy labor. Two pre-trained deep learning networks were used to preprocess these images without paying too much attention to algorithm developing and training data annotation. Afterward, we developed an automated workflow to calculate the 3D-based morphological traits and correct the top direction, which ensured the 3D models of the broccoli head can be used directly for the next step.

Parallel to the close-range pipeline, we also developed the aerial pipeline to obtain the 3D models of the full field by \gls{uav} photogrammetry. The key idea is to obtain the shape and position of each broccoli head in the field. But the resolution of photogrammetry-produced field maps or 3D models is not enough for accurate head segmentation. To improve the quality and labor for head segmentation, we first fused the raw \gls{uav} images, which have better resolution and quality, and the field map, which has unique geographical coordinates. Then, we fused the time-series data from different growing stages; the seedling stage was used to obtain the position and IDs of each broccoli, which is a very simple detection task; and these positions were used to narrow the image processing area for head segmentation. Lastly, labor-saving active learning was used to interactively annotate the training data for the segmentation deep learning model. The morphological traits of segmented broccoli heads were also calculated, which is fundamental for the final step.

Finally, we fused the close-range and aerial pipelines. By using the paired morphological traits data from those destructively sampled broccoli heads, we trained an \gls{automl} calibration model to further improve the accuracy of aerial head segmentation. Then, the accuracy of head center positions was also calibrated by the piecewise affine transformation. Finally, for each broccoli head from the aerial survey, the closest 3D model was matched and transformed from the close-range high-quality database. After placing those transformed 3D models back into the aerial model, a script was developed for 3D virtual visualization and a better understanding of the head growth status in the field.

Overall, the results of these three research chapters showed that the proposed pipelines including active learning, deep learning, backward projection, auto machine learning, template matching, and data fusion, led to improved performance in the tested broccoli fields from 2020 to 2022. The results and statistical analysis concluded that the research objectives have been achieved and all the source codes are published on GitHub for replication and any usage purposes, we can conclude that the research has made a positive contribution to the 3D-based plant phenotyping and precision agriculture for broccoli farmlands.

% limitations -> combine all chapters, and new points can be found?
Despite the promising results of the experiment in the broccoli field over three years, there is currently no strong evidence that the proposed methods can achieve similar performance in different farmland or vegetables. Since several assumptions of the methods were based on the characteristics of broccoli heads, such as the solid shape of broccoli head when vertically rotating on the rotation platform, small position variation from seeding to heading, the circular shape of the broccoli head, the clear color differences between the crown and stem for broccoli heads, and the flatten mushroom-shaped broccoli crown shape. Hence, the proposed full pipelines can not be applied directly to vegetables with variate shapes. The full pipelines may still work on the cauliflowers (just change the color from greenish to whitish); For vegetables like sweet potatoes and cabbages with very different shapes, some modules like close-range reconstruction or 3D-based morphological traits calculation may still work, but some kinds of modifications for other parts are expected. Another limitation of the proposed close-range and aerial data fusion is the visibility of targets in both pipelines, at least for the current stage, not directly applicable for sweet potatoes and potatoes whose targets are beneath the soil, which deserve further studies.

\section{Future research prospects}

% automation rotation platform problem
In this thesis, several new approaches or pipelines were proposed to improve the performance of 3D-based plant phenotyping. It was suggested to extend each of them in the future. As mentioned in Chapter 2, the dual-rotation approach for data collection still needs manually change and vertically flip plants, and only applicable to solid plants. A more automated instrument should be developed to further reduce manual operations. For example, by using a track controlled by a stepper motor or robotic arms to achieve automated plant replacement and rotation. For softer plant organs like leaves and flowers, the dual-rotation method may not work due to the structure changing, so the approach needs to be changed by setting up camera arrays at different angles to achieve similar results.

% nerf app
In addition, besides image-based photogrammetry, the latest \gls{nerf} technology is also worth exploring. There is now an application of \gls{nerf} based on the latest iPad's \gls{lidar} sensor for camera position estimation. In our preliminary experiments in early May 2023, especially for landscape plants with complex structures, better 3D models can be obtained than with photogrammetry. However, currently, the iPad needs to be manually moved to match the path guidance in the software. If this process can be interactively realized with a robotic arm, it also has broad potential applications.

% Virtual plants powered crop data analysis and phenotyping applications.
In Chapter 4, the data fusion between close-range and aerial surveys showed a good result in the visualization hidden part of the broccoli head, although based on statistical regression for visualization rather than actual status. As discussed in Chapter 4, for different cultivars of broccoli, more broccoli heads should be collected to provide a sufficient template database to meet more detailed transformation needs. However, the proposed method is facing challenges when dealing with crops with complex structures, such as maize or soybean canopies. On one hand, the severity of occlusion makes it very difficult to distinguish individual maize or soybean; on the other hand, due to the multilateral structure of crops, it is difficult to collect enough samples for template transformation. In this condition, the advent of procedural modeling points very different solutions which use several parameters to control the structure of 3D models. For example, The L-system was used to construct virtual crop models that adjusted the parameters to approximate the real crop by three-view photos \citep{cieslak_l-system_2021}. Although the model obtained using the above method is only structural, it is not realistic enough in terms of detail and appearance. \citet{mikami_hidden_2022} used photo-quality texture mapping on the plant 3D models with relatively simple geometries and finally obtained a very realistic 3D models in texture. In our preliminary experiment, a similar corn model using parameter controls has already been achieved in Blender, but due to the complexity of the research, the ability to repair the corn canopy has not yet been realized. In subsequent studies, we hope to develop a parameter-controlled broccoli model and complete the repair of the corn canopy.

% The plant and its canopy architecture influence the responses and interactions with various environmental factors, which ultimately affects the yield. The conventional analysis methods rely on heavy and repetitive field measured geometry traits along with statistical analysis through whole growing seasons. While the advent of digital twin technology points a very different future, the researchers can operate directly on the virtual plants and preview its impact on the plant immediately \citet{verdouw_digital_2021}. The fundamental in implementing such technology is to present the 3D crop model (virtual plant) accurately in the computer first, and then implement architecture fine-tuning and phenotyping applications on them. This seminar will summarize recent published papers on how this has been achieved.

% Currently, there are three main methods of acquiring virtual plant that can be architecture fine-tuned. 1) By template stitching. chang_3dcap_2022 obtained the database of wheat organ 3D models at different growth stages by destructive sampling and manual measurement of parameters. Then combine those organed to one wheat by random picking and placing with random position angles within the range obtained by real measurements. wen_3d_2021 applied the similar idea for maize. 2) By static deformation. The 3D static model of whole maize (in point cloud format) was obtained by 3D reconstruction. The maize model was then automatically segmented to organs, and these segmented parts were transformed by applying geometric transformations to implement changes in architecture liu_canopy_2021. 3) By parametric approximation. The L-system was used to construct a parameter adjustable virtual crop model and then adjusted the parameters to approximate the real crop by three-view photos cieslak_l-system_2021. This process was also reflected in the implementation of CG modelling. For example, the general shape of the plant was obtained through relatively simple geometries, and then photo-quality texture mapping was used to obtain a very realistic CG model mikami_hidden_2022.

Furthermore, several advanced phenotyping applications can be simulated on the abovementioned virtual 3D plant models. 1) The ray-tracing technologies can be applied to simulate lights within the canopy. 2) The point cloud simulation technology can be used to simulate point clouds from the virtual canopy. For the canopy traits which are easy to get from virtual plants but hard from the real field, the corresponding models between features from the simulated point cloud and traits from the virtual canopy can be trained and then applied to the field-scanned point cloud data to inverse the actual data \citep{liu_estimating_2017}. 3) the CG rendering techniques can decrease the workload of data annotating in the phenotyping data processing by deep learning. The rending engine can yield CG photos that are very close to the real world. At the same time, by adjusting the model texture to solid pure color, the corresponding labeled information for both CG photos \citep{mikami_hidden_2022} and point clouds \citep{chaudhury_3d_2020} can be generated in batch from random virtual canopies.
