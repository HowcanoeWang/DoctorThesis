\chapter{Destructive 3D phenotyping pipeline}

\section{Introduction}

Estimating the plant phenotypes accurately and efficiently can help to bridge the gap between genotype and phenotype. The traditional phenotyping measurement is time-consuming, laborious, and often not accurate. Although several authors have developed 2D image-based phenotyping methods which are more efficient, non-destructive, and have higher throughput \citep{yang_greenness_2015,guo_easypcc_2017,zou_broccoli_2019}, these approaches are unable to describe the plant 3D structure due to the occlusion and dimension loss when projecting onto the 2D plane. As a result, it produces inaccuracies and uncertainties for advanced phenotyping applications.

To overcome the drawbacks of 2D image-based phenotyping, several studies have paid attention to 3D approaches. \citet{paulus_measuring_2019} and \citet{kochi_introduction_2021} have summarized the current approaches to obtain 3D plant models; and a large number of studies have chosen the 3D reconstruction by photogrammetry using common RGB cameras due to the low device cost \citep{xiao_estimating_2021,zermas_3d_2020,zhang_estimating_2016}. The 3D model of an object can be calculated from images from different view angles with enough overlap (Fig.~\ref{fig:des1}). The full process includes structure-from-motion (SfM, to calculate relative positions between images and the rough 3D point cloud of the object), multi-view stereo (MVS, to densify the 3D point cloud of the object), and surface reconstruction (to obtain object 3D mesh model); for more details, please refer to this book \citep{hartley_multiple_2000} and this review \citep{snavely_scene_2010}. Although the proposed 3D image-based phenotyping was proved feasible for many agriculture applications, when apply to the broccoli head cases, the current 3D method is facing several challenges.

\input{figures/des.fig.1.tex}

The first challenge is image acquisition for cost-effective reconstruction. Several authors used the following three approaches to acquire images (Fig.~\ref{fig:des1}): (a) fix the object and fix cameras \citep{nguyen_structured_2015}; (b) fix the object and move a camera manually \citep{xiao_image-based_2020} or using robotic arms \citep{cao_quantifying_2019,nguyen_3d_2016}; and (c) rotate the object and fix or move the camera(s) \citep{kochi_3d_2018,gao_novel_2021}. The main drawback of these imaging approaches is that it is difficult to provide fully complete view angles of plants within acceptable cost. Most methods can only capture images of the sunny side (such as the front surface of leaves and the top of broccoli crowns), while capturing images of the shaded side (such as the back surface of leaves and the downside of broccoli crowns) is more difficult and requires extreme upward viewing angles (Fig.~\ref{fig:des1}.d). Even if such images can be acquired, they often fail to align with the 3D model due to insufficient feature matching points. Hence, a solution is required to improve the integrity of the reconstructed plant 3D model, especially for the solid closed harvestable organs like broccoli head.

The second challenge is image preprocessing for the acquired images. Compared to the approach of building the full scenario, segmenting the plant parts out \citep{ge_method_2019}, and denoising \citep{wu_mvs-pheno_2020}; some studies segmented and masked the foreground (plant) area before doing the 3D reconstruction \citep{nguyen_3d_2016,kochi_3d_2018}, to improve the model quality and eliminate the effect of background noises. Although the controlled environment can provide pure background and stable lighting, it is still challenging to develop a robust algorithm to segment the broccoli area perfectly, especially to handle the shadows caused by the irregular broccoli head shape at the different growing stages. Some outdoor studies showed the power of deep learning for plant area detection and segmentation \citep{zhou_monitoring_2020,blok_effect_2021,garcia_towards_2021}, but limited by the GPU memory, the original images often resized to smaller sizes for deep learning. For example, \citet{zhou_monitoring_2020} resized to $1440 \times 1080$, \citet{blok_effect_2021} resized to $1024 \times 1024$, and \citet{garcia_towards_2021} resized to $640 \times 480$. The plant mask produced in such a resolution can not fit well into the original images in detail (in our case, is $5184 \times 3456$), and hence also need a method to produce detailed masks with the original resolution.

In this study, we applied several strategies to overcome the previously mentioned challenges and provide a labor-saving pipeline for obtaining high-quality 3D models for destructive broccoli heads. The objectives of this study were to: (1) develop the dual-rotation object strategy with a fixed camera to collect images with abundant view angles for integrity 3D reconstruction; (2) implement the two-step deep learning workflow to acquire the detailed plant masks on the original images; (3) calculate the 3D morphological traits of broccoli head and crown; and (4) validate the estimated head size using the manual measurements. This pipeline also has great potential to be applied to other solid closed harvestable organs, such as potatoes, cauliflowers, and sweet potatoes, which are related to the profit directly. Meanwhile, using just a simple RGB camera and several low-cost commercial photographic equipments, makes it easier to be widely-spread use.


\section{Methods and Materials}

\subsection{Plant 3D model acquisition}

% plant materials, the information for fields and selective sampling

\subsubsection{Imaging device}

% figure2: 3D reconstruction devices
\input{figures/des.fig.2.tex}

\subsubsection{Image preprocessing}



\subsubsection{Batch 3D reconstruction}

% marker detection for one round and ignore for others

% the batch preprocessing 

\subsection{Phenotypic traits extraction}

% figure3: general workflow
\input{figures/des.fig.3.tex}

\subsubsection{Head segmentation}

\subsubsection{Upward direction correction}

\subsubsection{Traits calculation}


\subsection{Validation}

% field measurmenet

% r2 and rmse


\section{Results}

\subsection{Image preprocessing}

% figure4: unet + casadePSP results
% add more examples
\input{figures/des.fig.4.tex}

\subsection{Plant 3D model}

% figure5: comparison between real photo and photos + model 3 views
\input{figures/des.fig.5.tex}

\subsection{Traits extraction}

% figure6: head extraction
% change to all reulst + one demo
\input{figures/des.fig.6.tex}


\subsection{Validation}

\input{figures/des.fig.7.tex}

\section{Discussion}



\section{Conclusion}