\chapter{Destructive 3D phenotyping pipeline}

\section{Introduction}

Estimating the plant phenotypes accurately and efficiently can help to bridge the gap between genotype and phenotype. The traditional phenotyping measurement is time-consuming, laborious, and often not accurate. Although several authors have developed 2D image-based phenotyping methods which are more efficient, non-destructive, and have higher throughput \citep{yang_greenness_2015,guo_easypcc_2017,zou_broccoli_2019}, these approaches are unable to describe the plant 3D structure due to the occlusion and dimension loss when projecting onto the 2D plane. As a result, it produces inaccuracies and uncertainties for advanced phenotyping applications.

To overcome the drawbacks of 2D image-based phenotyping, several studies have paid attention to 3D approaches. \citet{paulus_measuring_2019} and \citet{kochi_introduction_2021} have summarized the current approaches to obtain 3D plant models; and a large number of studies have chosen the 3D reconstruction by photogrammetry using common RGB cameras due to the low device cost \citep{xiao_estimating_2021,zermas_3d_2020,zhang_estimating_2016}. The 3D model of an object can be calculated from images from different view angles with enough overlap (Fig.~\ref{fig:des1}). The full process includes structure-from-motion (SfM, to calculate relative positions between images and the rough 3D point cloud of the object), multi-view stereo (MVS, to densify the 3D point cloud of the object), and surface reconstruction (to obtain object 3D mesh model); for more details, please refer to this book \citep{hartley_multiple_2000} and this review \citep{snavely_scene_2010}. Although the proposed 3D image-based phenotyping was proved feasible for many agriculture applications, when apply to the broccoli head cases, the current 3D method is facing several challenges.

\input{figures/des.fig.1.tex}

The first challenge is image acquisition for cost-effective reconstruction. Several authors used the following three approaches to acquire images (Fig.~\ref{fig:des1}): (a) fix the object and fix cameras \citep{nguyen_structured_2015}; (b) fix the object and move a camera manually \citep{xiao_image-based_2020} or using robotic arms \citep{cao_quantifying_2019,nguyen_3d_2016}; and (c) rotate the object and fix or move the camera(s) \citep{kochi_3d_2018,gao_novel_2021}. The main drawback of these imaging approaches is that it is difficult to provide fully complete view angles of plants within acceptable cost. Most methods can only capture images of the sunny side (such as the front surface of leaves and the top of broccoli crowns), while capturing images of the shaded side (such as the back surface of leaves and the downside of broccoli crowns) is more difficult and requires extreme upward viewing angles (Fig.~\ref{fig:des1}.d). Even if such images can be acquired, they often fail to align with the 3D model due to insufficient feature matching points. Hence, a solution is required to improve the integrity of the reconstructed plant 3D model, especially for the solid closed harvestable organs like broccoli heads.

The second challenge is image preprocessing for the acquired images. Compared to the approach of building the full scenario, segmenting the plant parts out \citep{ge_method_2019}, and denoising \citep{wu_mvs-pheno_2020}; some studies segmented and masked the foreground (plant) area before doing the 3D reconstruction \citep{nguyen_3d_2016,kochi_3d_2018}, to improve the model quality and eliminate the effect of background noises. Although the controlled environment can provide pure background and stable lighting, it is still challenging to develop a robust algorithm to segment the broccoli area perfectly, especially to handle the shadows caused by the irregular broccoli head shape at the different growing stages. Some outdoor studies showed the power of deep learning for plant area detection and segmentation \citep{zhou_monitoring_2020,blok_effect_2021,garcia_towards_2021}, but limited by the GPU memory, the original images often resized to smaller sizes for deep learning. For example, \citet{zhou_monitoring_2020} resized to $1440 \times 1080$, \citet{blok_effect_2021} resized to $1024 \times 1024$, and \citet{garcia_towards_2021} resized to $640 \times 480$. The plant mask produced in such a resolution can not fit well into the original images in detail (in our case, is $5184 \times 3456$), and hence also need a method to produce detailed masks with the original resolution.

In this study, we applied several strategies to overcome the previously mentioned challenges and provide a labor-saving pipeline for obtaining high-quality 3D models for destructive broccoli heads. The objectives of this study were to: (1) develop the dual-rotation object strategy with a fixed camera to collect images with abundant view angles for integrity 3D reconstruction; (2) implement the two-step deep learning workflow to acquire the detailed plant masks on the original images; (3) calculate the 3D morphological traits of broccoli head and crown; and (4) validate the estimated head size using the manual measurements. This pipeline also has great potential to be applied to other solid closed harvestable organs, such as potatoes, cauliflowers, and sweet potatoes, which are related to the profit directly. Meanwhile, using just a simple RGB camera and several low-cost commercial photographic equipment, makes it easier to be widely-spread use.


\section{Methods and Materials}

The pipeline proposed in this study has two main parts, the workflow to acquire high-quality broccoli head 3D models and the workflow to measure the morphological traits of broccoli heads. For the first workflow, we developed an automatic imaging device using commercial photographic equipment, which extended the design of \citet{kochi_3d_2018} to dual-rotation of objects; then applied marker detection (software built-in function) and deep learning segmentation of plant masks to the collected images, as image preprocessing; afterward, we extended our previous batch scripts in the \citet{feldman_easydcp_2021} to automatically operate the 3D reconstruction on a large number of broccoli heads to get 3D model results. For the second workflow, similar to our previous work \citep{feldman_easydcp_2021}, we first developed the unsupervised algorithm to split broccoli crown and stem parts; then corrected the broccoli head top direction of the 3D models (as the z-axis positive direction); lastly, we calculated several 2D and 3D morphological traits of the broccoli head and segmented crown.

\subsection{Plant 3D model acquisition}

\subsubsection{Plant materials}

Field trials were conducted at the experimental farm of the Institute for Sustainable Agro-ecosystem Services (ISAS), Nishi-Tokyo, Tokyo, Japan ($35^\circ 43'$N, $139^\circ 32'$E) from October 2021 to April 2022. The plot sizes were approximately $3000 m^2$; The meteorological data during the growth period were collected by a local weather station and are shown in Table~\ref{tbl:des1}. The soil was tilled and leveled using Half-Soiler and a Rotary implement. Then, the initial fertilizer was mechanically spread and mixed thoroughly on 11 October 2021; around 11,000 broccolis (cultivar: suzuka[すずか]) were machine transplanted with a plant spacing of $35 cm$ and row spacing of $70 cm$ was done from October 14 to 16; and additional fertilizer was applied by hand on October 28. Herbicides, insecticides, and fungicides were applied as needed using mechanical spreaders.

\input{tables/des.tbl.1.tex}




\subsubsection{Imaging device}

% figure2: 3D reconstruction devices
\input{figures/des.fig.2.tex}

\subsubsection{Image preprocessing}



\subsubsection{Batch 3D reconstruction}

% marker detection for one round and ignore for others

% the batch preprocessing 

\subsection{Phenotypic traits extraction}

% figure3: general workflow
\input{figures/des.fig.3.tex}

\subsubsection{Crown segmentation}

\subsubsection{Upward direction correction}

\subsubsection{Traits calculation}


\subsection{Validation}

% field measurmenet

% r2 and rmse


\section{Results}

\subsection{Image preprocessing}

% figure4: unet + casadePSP results
% add more examples
\input{figures/des.fig.4.tex}

\subsection{Plant 3D model}

% figure5: comparison between real photo and photos + model 3 views
\input{figures/des.fig.5.tex}

\subsection{Traits extraction}

% figure6: head extraction
% change to all reulst + one demo
\input{figures/des.fig.6.tex}


\subsection{Validation}

\input{figures/des.fig.7.tex}

\section{Discussion}



\section{Conclusion}