\chapter{General Introduction}

\section{Fruits and vegetable production challenges}

Fruits and vegetables are important sources of trace elements, vitamins, and minerals in human nutrition. The \gls{who} recommends consuming at least 400 grams of fruits and vegetables per day to prevent malnutrition (\url{https://www.who.int/news-room/fact-sheets/detail/healthy-diet}). For example, broccoli (\textit{Brassica oleracea} L.) heads, which are one of the most important components of the global vegetable market, have shown their great potential to prevent cancer and cardiovascular disease by several case studies \citep{mahn_overview_2012,latte_health_2011}. However, their supply and quality can be easily affected by extreme weather events (\url{https://www.bbc.com/news/business-64776969}). Moreover, climate change and global warming also have negative impacts on vegetables, especially lettuce and broccoli, which need a cold accumulation period to produce a harvest \citep{bisbis_potential_2018}. In addition, different field management practices (e.g., tillage, density, and nutrients) also affect vegetable yield and quality \citep{jackson_onfarm_2004, satodiya_effect_2015}. Hence, it is important to monitor the plants during their growth stages and make proper management decisions promptly to ensure vegetable supply and quality.

Conventional agricultural management activities for such purposes require significant manual labor and are now facing several challenges. Firstly, they often require professional knowledge and may be subject to human error. Secondly, the availability of human labor in agriculture is currently decreasing due to long-term global events, such as urbanization and aging populations, as well as short-term pandemics, such as economic recessions and \gls{covid19} \citep{gallardo_adoption_2018, larue_labor_2020}. As a result, there is an unprecedented demand for labor-saving technologies. The development of technologies, such as automation, sensing, big data analysis, computer vision, and artificial intelligence, has made it possible to address such demands and thereby spark the digital revolution in the agricultural industry \citep{gallardo_adoption_2018}.

\section{Plant phenotyping techniques}
% background introduction for plant phenotyping
The labor-saving and digitized plant status monitoring, also known as plant phenotyping, has been rapidly developed and implemented in recent years \citep{araus_field_2014}. It is defined as ``\textit{the application of methodologies and protocols to measure specific traits related to plant structure or function with traits ranging from cellular to whole-plant levels}'' \citep{fiorani_future_2013, ghanem_physiological_2015}. Although different workflows are required for different crops and applications, the general workflow of plant phenotyping can be summarized as follows: 1) Data collection, which collects plant data using various sensors; 2) \gls{roi} extraction, which detects or segments plant parts at expected levels (e.g., full canopy or single organ) from the collected data; and 3) crop trait calculation and guidance for practical applications \citep{zhao_crop_2019}.

\subsection{Data collection}
%% platform types -> indoor, outdoor(ground, aerial, satellite)
For data collection, there are various sensors available for different purposes in plant phenotyping. These sensors can be categorized as environmental sensors (e.g. light intensity, temperature, and humidity) and plant sensors (e.g. organic compounds and plant images) \citep{garlando_plants_2020}. Environmental sensors are typically fixed in place as part of the \gls{iot} system to record critical environmental factors that affect plant phenotypes \citep{ghanem_physiological_2015}. On the other hand, plant sensors are usually mounted on platforms and moved around, as shown in Figure~\ref{fig:int1}. These platforms enable data collection at different scales, from the organ level to the individual and canopy levels, as demonstrated in both indoor (Figures~\ref{fig:int1}a-d) and outdoor (Figures~\ref{fig:int1}e-k) settings.

\input{figures/int.fig.1.tex}

% From the indoor to the outdoor, the commonly used platforms include self-designed indoor devices \citep{wu_mvs-pheno_2020,schunck_pheno4d_2021}, robotic arms in growth chamber or greenhouse \citep{chaudhury_machine_2018, du_greenhouse_2021}, tractor \citep{kusumam_3d_2017,blok_effect_2021}, ground vehicle \citep{liu_estimation_2017}, \gls{ugv} \citep{mcguire_high_2021,qiu_field-based_2019}, robotic outdoor instruments \citep{bai_nu-spidercam_2019, jin_exploring_2021}, \gls{uav} \citep{kierdorf_growliflower_2022, jang_review_2020}, and even satellite \citep{nguyen_monitoring_2020}. The scale of collected data using these platforms also changes from organ level, individual level to canopy level.

An important part of plant sensors is imaging sensors, which can record the morphological information of crops and are therefore commonly used in many plant phenotyping studies \citep{paulus_measuring_2019, feng_comprehensive_2021}. For example, infrared, multi- and hyper-spectral imaging sensors were used to calculate several vegetation indices \citep{han_modeling_2019}, such as \gls{ndvi}, to assess biomass \citep{jimenez-berni_high_2018}, yield, and water stress level \citep{herrero_yield_2020, romano_use_2011}, and to evaluate broccoli head freshness \citep{guo_evaluation_2022}. The common \gls{rgb} camera was also used to count plant number \citep{liu_estimating_2022}, plant density \citep{velumani_estimates_2021}, detect broccoli head \citep{blok_machine_2016}, and assess broccoli head quality \citep{stansell_use_2017}. These studies demonstrated the feasibility of using imaging sensors in plant phenotyping due to their high-efficiency and non-destructive nature.

%% 3D sensors
% introduction for 3D phenotyping, method to obtain 3D (passive, active) -> cost limit, choose RGB+SfM
However, these imaging sensors are difficult to use in directly describing the 3D morphological structure of plants, due to occlusion and dimension loss when projecting onto the 2D plane of photosensitive elements. As a result, inaccuracies and uncertainties arise when describing the 3D structure. With the development of sensing techniques, several studies have reviewed the latest available approaches for 3D plant phenotyping \citep{paulus_measuring_2019, okura_3d_2022, kochi_introduction_2021}. Figure~\ref{fig:int2} summarizes some of them, which are comprised of both active and passive scanners. A complete list of commercial 3D scanners is also provided by \citet[Table 5]{bartol_review_2021}.

\input{figures/int.fig.2.tex}

The active scanners use a light source to project on the object and analyze the reflection results to obtain the structure. It consists of two main parts: triangulation and \gls{tof}. The triangulation approach uses the shape changes on the projected straight laser line(s) to obtain the object structure. For example, \citet[Figure~3]{schunck_pheno4d_2021} used a laser line (Fig.~\ref{fig:int2}a) triangulation scanner (Perceptron Scan Works V5, Perceptron Inc., USA) to build a public 3D model dataset for maize and tomato. Structured light improves efficiency by projecting an array of lines which is often visible or infrared light, rather than laser (Fig.~\ref{fig:int2}b). The Microsoft Kinect V1 is one of the famous \gls{rgbd} cameras based on this approach, which was released in November 2010. However, it was used in only a few plant phenotyping studies \citep{nguyen_structured_2015}. Since the release of the second version (Kinect V2, in July 2014) and the third version (Azure Kinect, in March 2020) with a different \gls{tof} approach and better performance \citep{tolgyessy_evaluation_2021, lachat_assessment_2015}. 
The \acrfull{tof} measures the distance between the sensor and points on the object to obtain its structure. Depending on the type of light source used, it can be divided into two categories: infrared for \gls{tof} cameras (Fig.~\ref{fig:int2}c) and laser for \gls{lidar} (Fig.~\ref{fig:int2}d). Since the sun is a massive source of infrared, infrared-based ToF cameras typically do not perform well in outdoor environments with intense sunlight \citep{tolgyessy_evaluation_2021}. Therefore, they are commonly used for indoor plant reconstruction applications \citep{martinez_low_2019, zhang_3d_2020, xu_global_2023}. In contrast, lasers are often more tolerant to sunlight due to their higher energy density, making them widely used in outdoor environments. There are different types of \gls{lidar} sensors used in 3D plant phenotyping applications. These include 2D \gls{lidar} scanners \citep{garrido_3d_2015}, 3D \gls{hls} \citep{ma_calculation_2019}, 3D \gls{tls} \citep{wu_accurate_2019, su_estimation_2018, qiu_field-based_2019}, and 3D \gls{als} \citep{ten_biomass_2019, nguyen_uav_2023}.

The passive scanners rely on analyzing passively received image groups, mainly using \gls{rgb} images. The light field camera takes a group of photos with different focal lengths, and the object structure is calculated according to different degrees of clarity caused by the different distances from the sensor (Fig.~\ref{fig:int2}e). For example, \citet{apelt_phytotyping_2015} built a light field camera system for measuring morphological traits related to plant growth. A commercial low-cost light field camera (Lytro LF, Lytro Inc., USA) was used as an \gls{rgbd} camera to monitor maize 3D morphological traits \citep{schima_imagine_2016}. Another commercial \gls{rgbd} camera, Intel RealSense L515(Intel Corporation, USA), uses a stereo vision approach (Fig.~\ref{fig:int2}f). It uses binocular vision like human eyes to obtain the object's structure. While for other RealSense models (D455, D435i, and D415) integrate the \gls{rgb} camera with \gls{lidar} \citep{bartol_review_2021}. \citet{blok_image_2021} used this RealSense \gls{rgbd} camera to estimate the broccoli head size with different degrees of occlusion, which is a tough problem for common \gls{rgb} cameras. The photogrammetry approach is based on \gls{rgb} images obtained from different perspectives (view angles) using common imaging sensors (Fig.~\ref{fig:int2}g). It first uses the overlapped area among images to estimate the camera poses and object's rough 3D structure (tie points), called \acrfull{sfm}. Then the \gls{mvs} is applied to densify the 3D point cloud of the tie points, and surface reconstruction and texture rendering are performed to obtain a 3D mesh model of the objects. For more details, please refer to \citep{hartley_multiple_2000, snavely_scene_2010}.

% The structure from motion
%% software SfM
% ground sfm {Wu_MVS-Pheno_2020, Wang_Maize_2019, zhu_quantification_2020}; aerial sfm: {Liu_field-based_2021}
Unlike the previously mentioned scanners (Figures~\ref{fig:int2}a-f), which often require special, not-so-low-cost devices, the \gls{sfm} approach only requires an \gls{rgb} camera and \gls{sfm} software, with very flexible cost advantages. For the camera/sensor parts, even the cameras of smartphones at hand can be used to obtain photos for plant modeling \citep{li_measuring_2020}. If 3D plant models with ultra-high quality are required, a \gls{dslr} camera (over 4K resolution) can also be used to raise the 3D model quality to a higher level \citep{nguyen_3d_2016, drofova_use_2023}, often exceeding the resolution of previously commercial \gls{rgbd} cameras (around 1080p resolution). Meanwhile, a large number of \gls{sfm} open-source and commercial software packages are available for individual plants (close-range) and geo-referenced canopy (aerial) applications (Table~\ref{tbl:int1}). For this reason, many studies have used this approach to obtain 3D models of indoor individual plants \citep{wu_mvs-pheno_2020, zhou_automated_2019}, in-field individual plants \citep{jay_field_2015, herrero_structural_2023}, and in-field canopy \citep{kim_modeling_2018, herrero_canopy_2020}.

\input{tables/int.tbl.1.tex}

Recently, the \gls{cg} industry, which often needs to scan objects to create 3D models and render images from different perspectives, was introduced to a novel deep learning method by \citet{mildenhall_nerf_2022} (Fig.~\ref{fig:int2}h). The proposed approach, called \gls{nerf}, is based solely on the \gls{sfm} step and can bypass the time-consuming \gls{mvs}, 3D meshing, and 3D rendering steps required by photogrammetry. The official demo (\url{https://www.matthewtancik.com/nerf}) demonstrated the feasibility of obtaining plant 3D structures, and \citet{jignasu_plant_2023} obtained the maize 3D structure using this approach. However, the effectiveness of this approach relies on the quality of training data, and while there are many datasets available for industrial applications, there are not many for agricultural areas. As a novel technology, its application in agriculture still requires further development.

% Considering the maturity of technology and the cost of sensors, the low-cost and mature photogrammetry approach (Fig.~\ref{fig:int2}g) has been chosen to collect research data in this study.

% \subsection{Data analysis for phenotyping}
% for data analysis.

\subsection{ROI extraction}

After collecting the data, the next step is to extract the \acrfull{roi} for further analysis. Different research purposes may have different interpretations of \gls{roi}. For canopy-level studies or breeding studies, the \gls{roi} refers to the regions within each plot boundary \citep{trevisan_htp_2020,han_drone_2021}. For individual-level studies, the \gls{roi} is the plant parts without any background such as soil or weeds \citep{ge_method_2019,guo_fieldbased_2020}. For organ-level studies, the \gls{roi} refers to the part of each organ without other parts of the plant, such as the broccoli head \citep{zhou_monitoring_2020} and sorghum tassel \citep{ghosal_weakly_2019} without leaves. This section summarizes different methods and algorithms for image analysis and 3D data analysis, including data processing, 2D-based image analysis from imaging sensors, and 3D-based point cloud analysis from 3D photogrammetry or \gls{lidar} scanner.

\subsubsection{Data preprocessing} \label{sec:prepro}

% plot region extraction (canopy level)
% easympe, other plot segments
For canopy-level or full-field phenotyping applications, the \gls{dom} is often used as the field plot map (by \gls{uav} photogrammetry); the 3D point cloud is often used as 3D canopy models (by \gls{uav} photogrammetry or \gls{lidar} scanner). However, processing the whole file image or 3D data directly is often impossible, since the file size of these whole canopy data is very large (often over 0.5GB and can reach 10GB+). One common data preprocessing step in plant phenotyping applications is to split the whole field into smaller parts \citep{wang_easyidp_2021}. It can be split into equal-size grids (e.g. \citet{bauer_combining_2019} split the full \gls{dom} image into several 250$\times$250 pixel grids) or by the boundary of each (micro) plot \citep{tresch_easympe_2019}. The latter option is more meaningful and often used.

To manually place plot boundaries and their labels on the \gls{dom} map, \gls{gis} software such as ArcGIS Desktop (Esri, Redlands, USA) and QGIS (open source, \url{https://qgis.org}) are often utilized. (Micro) plot results are typically saved in shapefile format (*.shp) for better compatibility with other software. However, manually editing and operating software \gls{gui} can be time-consuming for large fields. Consequently, several studies have attempted to solve automatic \gls{roi} detection and generation by using computer vision. \citep{tresch_easympe_2019} developed an open-source Python package called EasyMPE for the semi-automatic generation and cropping of \gls{roi}. Subsequently, this tool was re-built using C++ by \gls{naro} and renamed PREPs (\url{http://cse.naro.affrc.go.jp/aitoh/PREPs}) for higher performance on the Windows platform. \citet{chen_grid_2020} also developed a Python-based tool called GRID (\url{https://zzlab.net/GRID}); \citet{mortensen_drone_2019} used MATLAB to create such a tool, and \citep{sara_automatic_2021} extended these options by introducing the capability to slightly adjust the \gls{roi} location for better canopy fitting. All of these tools also support cropping the entire \gls{dom} into smaller parts within the \gls{roi} for post-processing.

For the 3D canopy point cloud, point cloud processing software such as MeshLab (open source, \url{https://www.meshlab.net}) or CloudCompare (open source, \url{https://www.cloudcompare.org}) is often used for similar tasks of cropping \gls{roi}, but it is not very convenient. Instead, it is more common to use the \gls{roi} shapefiles created in the previous step and write batch scripts for processing. For example, \citet{sun_field_2018} used a Matlab script to crop the \gls{roi} on \gls{lidar} point cloud, but they did not publish their code. Therefore, the EasyIDP python package was developed for easier processing of \gls{dom} and point cloud \citep{wang_easyidp_2021}. Even after segmenting the point cloud into smaller parts, point downsampling and noise removal are often necessary to reduce data size and processing difficulties due to the disorderliness of the point cloud's data structure \citep{ma_calculation_2019}.

After preprocessing the full field into smaller parts and data sizes, the following ``2D image analysis'' and ``3D data analysis'' can be applied to each part.

\subsubsection{2D-based approaches}

Most of the image analysis tasks for plant phenotyping can be summarized into three categories: classification, detection, and segmentation. The classification task involves determining the class of objects present in an image. For example, deciding whether a broccoli head is healthy or diseased \citep{garcia_towards_2021}. The detection task combines classification and localization to identify the kind of objects and their locations in an image. For instance, detecting the number of maize buds \citep{liu_estimating_2022} or sorghum tassels \citep{ghosal_weakly_2019} in an image. The segmentation task aims to separate an image into distinct regions with particular shapes and borders. It can be further divided into semantic segmentation (where a single label is assigned to all objects belonging to a particular class, e.g., all the regions of plants or broccoli heads in an image) and instance segmentation (where unique labels are assigned to each object, e.g., the regions of each broccoli head in an image). These tasks (classification, detection, and segmentation) are not independent of one another, as semantic segmentation often involves classifying each pixel to obtain the segmentation result of the entire image \citep{guo_easypcc_2017}, while instance segmentation usually requires combining the results of semantic segmentation and detection to identify each object's region \citep[see Fig.~2]{luling_using_2021}.
%%%%%%%%%%%%%%%%%
% AI to here

% [todo] a figure about broccoli detection, sematic segmentation, and instance segmentation.

%%% traditional CV method
The most conventional attempts are trying to use computer vision algorithms on common \gls{rgb} images. In some simple cases, a manually defined color threshold can be used for separating plants from backgrounds. For example, \citet{choudhury_holistic_2018} converted the maize plant image from \gls{rgb} to \gls{hsv} and used ``\textit{hue (range 0.051-0.503), saturation (range: 0.102-0.804) and value (range 0.000-0.786)}'' to separating plant regions from the background. However, in most outdoor images with complex lighting and background conditions, the manual thresholding does not have good performance. Thus, \citet{meyer_verification_2008} proposed the color vegetation indices (NDI, ExG and ExR) and combined with \citet{otsu_threshold_1979} thresholding algorithms for automated crop imaging application. For better compatibility with more complex scenarios without being limited to green plants, \citet{guo_easypcc_2017} manually annotated the training data to train machine learning classifiers based on extended color information. Further, \citet{zou_broccoli_2019} and \citet{blok_machine_2016} segmented the broccoli buds and heads, respectively, using the texture and color information with the trained \gls{svm} classifier.

%%% deep learning based method
%% -> 深度学习2D图像检测算法 siyuan
In the same way, deep learning techniques show great potential for better RGB image processing results. The \gls{cnn} is a famous deep learning framework for color imagery used by many phenotyping studies. \citet{ghosal_weakly_2019} used a pre-trained \gls{cnn} model for sorghum head detection and counting; \citet{liu_estimating_2022} applied the Fast \gls{rcnn} for counting the maize seedling number; \citet{bender_high_2020} applied the FastRCNN for detecting and segmenting the entire broccoli on the RGB images and shared them as public datasets; \citet{blok_effect_2021} simplified the MaskRCNN with data augmentation for broccoli head segmentation on RGB images. However, its accuracy is affected by the leaf occlusion; \citet{blok_image_2021} fixed this issue by using the \gls{orcnn} to recover the hidden head area; and \citet{garcia_towards_2021} also applied the FasterRCNN for distinguishing the immature and diseased broccoli. 
 
% convetional cv solution, but algriculutre also has many alternatives. (depth info, time-series info, hyperspectal)

\subsubsection{3D-based approaches}

Although 2D-based machine learning and deep learning approaches show the feasibility of extracting \gls{roi}s, for very complex scenes it often requires labeling a considerable number of training sets to ensure a better performance. The 3D-based approaches can not only provide more morphological information but also can greatly simplify complex tasks.

The depth information is often used as a common and simple 3D-based approach. The depth image is a common file format for that information, which uses pixels to record depth values. It can be obtained from either depth cameras (often for close-range, the depth value is the distance from the object to the camera) or photogrammetry (often for aerial, the depth value is the height or altitude). Such information has been widely used for better plant segmentation for phenotyping studies. For close-range applications, \citep{luling_using_2021} used the depth image generated by photogrammetry and the color information to segment cabbage instances. For the aerial photogrammetry applications, \citet{guo_fieldbased_2020} segmented the plant area inside each \gls{roi} using the color and the depth information from \gls{dom} and \gls{dsm} respectively. 

Another format for recording the depth information is the 3D point cloud, and the 3D point cloud analysis is also of great importance for the 3D-based approach. The general processing workflow for its analysis includes: 1) data preprocessing to decrease the data size (see Subsection \ref{sec:prepro}); 2) semantic segmentation for separating foreground (plants) and background (soils etc.,); 3) instance segmentation of each plants for individual-level studies; and 4) instance segmentation of each organ for organ-level studies. The conventional methods are introduced first and then step to deep learning approaches like 2D-based approaches.

For semantic segmentation between plants and backgrounds. Color thresholding mentioned in the 2D-based approach also works for 3D point clouds with colors. \citet[Fig.~3]{xiao_image-based_2020} manually defined \gls{rgb} threshold ($R-G\geq7$) for separating plant points from backgrounds like shadow and soil points. The geometry relationship among points can also be used for segmentation. \citet{ge_method_2019} applied the \gls{gmm} clustering to recognize the points of broccoli buds and remove the discrete points (noises) by \gls{knn} algorithm. While instead of extracting plant parts, \citet{garrido_3d_2015} used the \gls{ransac} plane regression to find out the flat ground points and removed them.

After semantic segmenting of the plant part, the instance segmentation to split each plant should proceed for the individual-level studies. \citet{hofle_radiometric_2014} developed a region-growth method based on the local maxima in elevation to split each maize bud from semantic segmented points. It shared a similar idea of \gls{dbscan} clustering method which is used by several studies later for maize \citep{lin_segmentation_2022} and cotton balls \citep{sun_3d_2020}. \citet{kusumam_3d_2017} segmented broccoli heads by Euclidean clustering method and the \gls{svm} classifier on the 3D features and \citet{montes_real-time_2020} further improved is effiency to almost real-time.

Finally, the organ-level studies require segmentation of each organ, especially for crops with complex morphological structures (e.g. maize and tomato). According to the morphological characteristics of different crops or cultivars, different studies proposed different solutions. For the tasks of maize leaf segmentation, \citet{jin_stemleaf_2019} proposed a \gls{mnvg} algorithm; \citet{liu_canopy_2021} used skeletonize and region growth; \citet{wang_dfsp_2023} proposed a distance field-based segmentation pipeline; and \citet{miao_label3dmaize_2021} used the optimal transportation distance and published an interactive segmentation tool (Label3Dmaize, \url{https://github.com/syau-miao/Label3DMaize}). For the tomato stem and leaf segmentation, \citet{rossi_implementation_2022} proposed a complex but automatic workflow using Matlab, including radius detection, sphere climbing, and phyllotaxy retrieving. \citet{helin_using_2023} proposed a t-distributed stochastic neighbor embedding segmentation method and proved its feasibility on five different plants (hibiscus, maple, tomato, tobacco, and rosebush). \citet{dutagaci_rose-x_2020} and \citet{schunck_pheno4d_2021} even published an annotated dataset for evaluating 3D plant organ segmentation methods.

One limitation of the conventional 3D-based analysis method, like the 2D-based image analysis method mentioned above, is the necessity to develop specific algorithms based on the characteristics and objectives of different plants. Existing algorithms often have low compatibility with new crops and objectives. The 2D-based deep learning approach shows the possibility of changing the difficulties from specific algorithm development to training data annotation. But unlike the well-structured 2D raster image data ($m \times n$ matrix, Fig.~\ref{fig:int3}b), it is difficult to apply the neural networks directly on the 3D point cloud data with disordered structures (Fig.~\ref{fig:int3}d).

\input{figures/int.fig.3.tex}

Currently, there are four solutions for applying deep learning on the 3D point cloud. 
The first solution is 3D convolution, which converts the disordered point cloud (Fig.~\ref{fig:int3}d) to the ordered voxel model (Fig.~\ref{fig:int3}e). One famous deep learning network based on this idea is the VoxNet \citep{Maturana_VoxNet_2015}, but it is easily affected by the resolution (sidelength of the voxel) selection and it is also limited with small data sizes (maximum $32 \times 32 \times 32$ voxel numbers). \citet{jin_separating_2020} developed a \gls{vcnn} for maize stem and leaf classification and segmentation. 
The second solution is graph convolution, which converts the disordered point cloud to the ordered graph (Fig.~\ref{fig:int3}c\&f). The graph relationship already supports distinguishing different shape features like leave (flat plane) and stem (long cylinder) by the self-defined rule \citep{mirande_graph-based_2022}. But it is also possible to apply the \gls{gcn} \citep{wu_comprehensive_2021,zhou_graph_2020} or \gls{dgcnn} \citep{phan_dgcnn_2018} on the graph data. \citet{du_pst_2023} tested the feasibility of \gls{dgcnn} on the instance segmentation on rapeseed leaves.
The third solution is converting multi-view projection onto 2D images for 2D-based detection or segmentation, and re-projecting the results back to 3D point cloud. One famous network for this idea is the \gls{mvcnn} proposed by \citet{su_mvcnn_2015}. \citet{jin_deep_2018} applied this idea for segmenting individual maize from canopy 3D point cloud. While \citet{van_plant_2019} used the geometry relationship between 3D models and the raw images from photogrammetry, to project the 2D deep learning results on the raw images back to the tomato 3D point cloud models.
The last solution is applying deep learning on the point cloud directly. \citet{qi_pointnet_2016} first trained a deep learning network called PointNet by randomly shuffling the order to avoid the impact of the order of points in point clouds. However, the algorithm complexity is factorial N, which becomes unacceptable for cloud sizes above 1000 points. Thus, an updated version PointNet++ with significantly better performance was proposed to solve the lack of hierarchical feature aggregation in the previous version \citep{qi_pointnet_2017}. \citet{boogaard_boosting_2021} tested its feasibility on the cucumber segmentation. And the PointNet++ is also acting as a comparison standard by several newly proposed networks for plants \citep{jin_separating_2020,zhou_automated_2022,du_pst_2023,li_psegnet_2022}. To be more specific, \citet{zhou_automated_2022} and \citet{du_pst_2023} proposed \gls{pct}-based networks for broccoli head segmentation and rapeseed leaves segmentation, respectively. \citet{li_plantnet_2022} proposed a PlantNet and its updated version PSegNet \citep{li_psegnet_2022} for the organ segmentation of sorghum, tomato, and tobacco.

\subsection{Traits calculation}

After segmenting the \gls{roi}, several different kinds of traits calculation can be applied to the 2D-based and 3D-based results. \citet[Table 1]{du_greenhouse_2021} summarized the 2D-based traits on common RGB images to six categories: 1) geometric (morphological) traits like projected area, projected perimeter, convex area, convex perimeter, circumcircle diameter, the angle of main axis, the length of long and short axis; 2) structure traits like the length or proportion of intersection line segments in concentric circles; 3) color space traits like mean and variance of Red in RGB; 4) color index traits like \gls{exg}, \gls{vdvi}, \gls{rgri}; 5) color component traits like 1st order color moment of B component; 6) texture traits like contrast, dissimilarity, and homogeneity. Among them, geometric (morphological) traits, color index traits, and texture traits are often used in other studies. For example, \citet{stansell_use_2017} extracted the texture traits like head color, head smoothness, head size, head uniformity as broccoli head quality indices; \citet{bauer_combining_2019} applied the color index traits, \gls{ndvi}, as an indicator for lettuce maturity. 

When it comes to 3D-based approaches, except the aforementioned 2D morphological traits like projected canopy area, convex and concave hull area, the canopy volume and canopy height is also available for the cotton canopy \citep{jiang_quantitative_2018}. For the maize canopy applications, other canopy traits like maize height \citep{hammerle_mobile_2018,qiu_field-based_2019} and row spacing \citep{qiu_field-based_2019} can also be calculated. \citet[Table 3]{jin_non-destructive_2020} calculated the morphological traits of maize from canopy-level to leaf organ-level, including the height, canopy cover, plant area index, projected leaf area, volume, stem diameter, leaf length and width. \citet{itakura_automatic_2018} obtained the leaf inclination angle and \citet{liu_canopy_2021} analyzed its effect on the canopy light use efficiency. And the 3D-based traits can also support some advanced applications for drought stress \citep{su_evaluating_2019, sorrentino_lettuce_2020}, intercropping \citep{liu_field-based_2021} and light competition \citep{zhu_quantification_2020} analysis.

% [todo] skeleton extraction.
% \subsection{Advanced applications}
%% 时间序列分析的文献(使用时间序列追踪参数的变化) 思源笔记
% they were used for discovering the loci regulating flower opening [11\citep{han_drone_2021}].
%% 收期预测、光照模拟, 辅助QTL

\section{Challenges in plant phenotyping}

%% 上述的三维分析方法，仅限于高质量的点云？对于大部分来说，质量不够很难进行3D的分析

%% 数据集标注的工作量问题
Besides, the model selection and parameter adjustment of machine learning as well as the training data preparation of machine learning, are still highly dependent on expert experiences and intensive labor. The auto machine learning framework and weakly supervised learning have merged as the interesting candidate to solve such dilemma.

\section{Objective of this study}

\begin{enumerate}
    \item To develop an almost-automatic 3D reconstruction workflow that can obtain the integrate and high-quality 3D models of destructively sampled broccoli heads.
    \item To develop an unsupervised phenotyping workflow that can automatically segment the broccoli crown part from Objective 1, and calculate several 1D to 3D morphological traits.
    \item To develop an improved workflow for \gls{uav}-based 3D reconstruction broccoli canopy using the 3D to 2D projection and labor-saving deep learning technique, which can obtain better 2D morphological traits of broccoli heads in complex outdoor conditions.
    \item To combine the strengths of the \gls{uav}-based pipeline (high throughput but low quality) and the destructive-based pipeline (high quality but low throughput). Using the auto machine learning regression and the template matching to recover the 3D traits from \gls{uav}-based 2D traits for all broccolis in the field.

\end{enumerate}


\section{Outline of this study}

Chapter 1 is an overview of the study's background information, relative studies, and objectives

Chapter 2 develops and validates the 3D phenotyping pipeline for destructively sampled broccoli heads using the photogrammetry technique, which includes obtaining high-quality plant 3D models and calculating the 3D traits.


Chapter 3 develops and validates the 3D phenotyping pipeline for \gls{uav} sensing on broccoli canopy, includes the 3D to 2D projection pipeline to improve deep-learning-based phenotyping

Chapter 4 tests the idea of cross-scale assimilation of broccoli based on the pipeline built in Chapter 2 and Chapter 3.

Chapter 5 summarizes the general conclusions of this study, and also discusses the research prospects in the future.